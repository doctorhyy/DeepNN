{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18edd63f",
   "metadata": {},
   "source": [
    "# 首先导入数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa63036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SurvivalDataset(Dataset):\n",
    "    ''' The dataset class performs loading data from .h5 file. '''\n",
    "    def __init__(self, h5_file, is_train):\n",
    "        ''' Loading data from .h5 file based on (h5_file, is_train).\n",
    "\n",
    "        :param h5_file: (String) the path of .h5 file\n",
    "        :param is_train: (bool) which kind of data to be loaded?\n",
    "                is_train=True: loading train data\n",
    "                is_train=False: loading test data\n",
    "        '''\n",
    "        # loads data\n",
    "        self.X, self.e, self.y = \\\n",
    "            self._read_h5_file(h5_file, is_train)\n",
    "        # normalizes data\n",
    "        self._normalize()\n",
    "\n",
    "        print('=> load {} samples'.format(self.X.shape[0]))\n",
    "\n",
    "    def _read_h5_file(self, h5_file, is_train):\n",
    "        ''' The function to parsing data from .h5 file.\n",
    "\n",
    "        :return X: (np.array) (n, m)\n",
    "            m is features dimension.\n",
    "        :return e: (np.array) (n, 1)\n",
    "            whether the event occurs? (1: occurs; 0: others)\n",
    "        :return y: (np.array) (n, 1)\n",
    "            the time of event e.\n",
    "        '''\n",
    "        split = 'train' if is_train else 'test'\n",
    "        with h5py.File(h5_file, 'r') as f:\n",
    "            X = f[split]['x'][()]\n",
    "            e = f[split]['e'][()].reshape(-1, 1)\n",
    "            y = f[split]['t'][()].reshape(-1, 1)\n",
    "        return X, e, y\n",
    "\n",
    "    def _normalize(self):\n",
    "        ''' Performs normalizing X data. '''\n",
    "        self.X = (self.X-self.X.min(axis=0)) / (self.X.max(axis=0)-self.X.min(axis=0))\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        ''' Performs constructing torch.Tensor object'''\n",
    "        # gets data with index of item\n",
    "        X_item = self.X[item] # (m)\n",
    "        e_item = self.e[item] # (1)\n",
    "        y_item = self.y[item] # (1)\n",
    "        # constructs torch.Tensor object\n",
    "        X_tensor = torch.from_numpy(X_item)\n",
    "        e_tensor = torch.from_numpy(e_item)\n",
    "        y_tensor = torch.from_numpy(y_item)\n",
    "        return X_tensor, y_tensor, e_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcefbaa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> load 7098 samples\n",
      "=> load 1775 samples\n",
      "Training dataset length: 7098\n",
      "X_sample shape: torch.Size([14])\n",
      "y_sample shape: torch.Size([1])\n",
      "e_sample shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 定义数据文件路径\n",
    "h5_file = './data/support/support_train_test.h5'\n",
    "#h5_file = './data/metabric/metabric_IHC4_clinical_train_test.h5'\n",
    "\n",
    "\n",
    "# 创建训练集数据集实例\n",
    "train_dataset = SurvivalDataset(h5_file, is_train=True)\n",
    "test_dataset = SurvivalDataset(h5_file, is_train=False)\n",
    "# 可选：如果需要，你可以查看数据集的长度\n",
    "print(\"Training dataset length:\", len(train_dataset))\n",
    "\n",
    "# 可以通过索引访问数据集中的数据\n",
    "# 假设想访问第一个样本的数据\n",
    "X_sample, y_sample, e_sample = train_dataset[0]\n",
    "\n",
    "# 打印样本数据的形状（假设m为特征的维度）\n",
    "print(\"X_sample shape:\", X_sample.shape)  # 应该是 (m,)\n",
    "print(\"y_sample shape:\", y_sample.shape)  # 应该是 (1,)\n",
    "print(\"e_sample shape:\", e_sample.shape)  # 应该是 (1,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65206a7f",
   "metadata": {},
   "source": [
    "## train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fab76663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "X_batch shape: torch.Size([7098, 14])\n",
      "y_batch shape: torch.Size([7098, 1])\n",
      "e_batch shape: torch.Size([7098, 1])\n"
     ]
    }
   ],
   "source": [
    "# 定义批次大小（batch size）\n",
    "#batch_size = 32\n",
    "\n",
    "# 创建训练集数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_dataset.__len__(), shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_dataset.__len__(), shuffle=True)\n",
    "# 遍历数据加载器中的每一个批次\n",
    "for batch_idx, (X_batch, y_batch, e_batch) in enumerate(train_loader):\n",
    "    # 在这里执行训练代码，例如：\n",
    "    # optimizer.zero_grad()\n",
    "    # outputs = model(X_batch)\n",
    "    # loss = criterion(outputs, y_batch, e_batch)\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "    \n",
    "    # 可以根据需要打印每个批次的数据形状\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(\"X_batch shape:\", X_batch.shape)  # 应该是 (batch_size, m)\n",
    "    print(\"y_batch shape:\", y_batch.shape)  # 应该是 (batch_size, 1)\n",
    "    print(\"e_batch shape:\", e_batch.shape)  # 应该是 (batch_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceba0023",
   "metadata": {},
   "source": [
    "## test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7ab2784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "X_batch shape: torch.Size([1775, 14])\n",
      "y_batch shape: torch.Size([1775, 1])\n",
      "e_batch shape: torch.Size([1775, 1])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (X_test_batch, y_test_batch, e_test_batch) in enumerate(test_loader):\n",
    "    # 在这里执行训练代码，例如：\n",
    "    # optimizer.zero_grad()\n",
    "    # outputs = model(X_batch)\n",
    "    # loss = criterion(outputs, y_batch, e_batch)\n",
    "    # loss.backward()\n",
    "    # optimizer.step()\n",
    "    \n",
    "    # 可以根据需要打印每个批次的数据形状\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(\"X_batch shape:\", X_test_batch.shape)  # 应该是 (batch_size, m)\n",
    "    print(\"y_batch shape:\", y_test_batch.shape)  # 应该是 (batch_size, 1)\n",
    "    print(\"e_batch shape:\", e_test_batch.shape)  # 应该是 (batch_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8548f98",
   "metadata": {},
   "source": [
    "## C_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67f520f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines.utils import concordance_index\n",
    "def c_index(risk_pred, y, e):\n",
    "    ''' Performs calculating c-index\n",
    "\n",
    "    :param risk_pred: (np.ndarray or torch.Tensor) model prediction   模型预测\n",
    "    :param y: (np.ndarray or torch.Tensor) the times of event    事件e的时间\n",
    "    :param e: (np.ndarray or torch.Tensor) flag that records whether the event occurs   标记，记录事件是否发生\n",
    "    :return c_index: the c_index is calculated by (risk_pred, y, e)   返回计算的c指数\n",
    "    '''\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        y = y.detach().cpu().numpy()\n",
    "    if not isinstance(risk_pred, np.ndarray):\n",
    "        risk_pred = risk_pred.detach().cpu().numpy()\n",
    "    if not isinstance(e, np.ndarray):\n",
    "        e = e.detach().cpu().numpy()\n",
    "    return concordance_index(y, risk_pred, e)  # 直接存在计算c指数的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68807fe1",
   "metadata": {},
   "source": [
    "## 随机生存森林"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cffea91",
   "metadata": {},
   "source": [
    "随机生存森林预测的结果是生存时间，所以在进行C_index预测的时候，为了和risk_predict区别开来需要理解为 生存时间越长 所对应的生存风险越小。所以实际上这里不需要加负号，但是在deepSurv中需要给数据加上负号来得到对应于生存时间的生存风险"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab80e2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomSurvivalForest(min_samples_leaf=15, min_samples_split=10, n_jobs=-1,\n",
      "                     random_state=42)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.util import Surv\n",
    "\n",
    "# 假设你已经有了X_batch, y_batch和e_batch\n",
    "# 将数据从张量转换为numpy数组\n",
    "X_batch_np = X_batch.numpy()\n",
    "y_batch_np = y_batch.numpy().flatten()\n",
    "e_batch_np = e_batch.numpy().flatten()\n",
    "\n",
    "# 创建生存数据结构\n",
    "# Surv函数接受两个参数：事件指示（事件发生为True，未发生为False）和生存时间\n",
    "surv_data = Surv.from_arrays(event=e_batch_np, time=y_batch_np)\n",
    "\n",
    "# 将特征矩阵转换为DataFrame\n",
    "X_batch_df = pd.DataFrame(X_batch_np)\n",
    "\n",
    "# 训练RSF模型\n",
    "rsf = RandomSurvivalForest(n_estimators=100, min_samples_split=10, min_samples_leaf=15, n_jobs=-1, random_state=42)\n",
    "rsf.fit(X_batch_df, surv_data)\n",
    "\n",
    "# 打印模型信息\n",
    "print(rsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea5df36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6123539179633075"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将测试数据从张量转换为numpy数组\n",
    "X_test_batch_np = X_test_batch.numpy()\n",
    "\n",
    "# 将特征矩阵转换为DataFrame\n",
    "X_test_batch_df = pd.DataFrame(X_test_batch_np)\n",
    "\n",
    "# 使用训练好的RSF模型进行预测\n",
    "# predict_survival_function 返回的是一个生成器\n",
    "surv_funcs = rsf.predict_survival_function(X_test_batch_df)\n",
    "\n",
    "# 定义你希望使用的生存概率阈值\n",
    "p = 0.5\n",
    "\n",
    "# 计算每个样本的指定生存概率时的生存时间\n",
    "# 例如，生存概率为p时的时间点\n",
    "specified_surv_times = np.array([np.interp(p, sf.y[::-1], sf.x[::-1]) for sf in surv_funcs])\n",
    "tensor = torch.tensor(specified_surv_times, dtype=torch.float32)\n",
    "valid_c = c_index(specified_surv_times,y_test_batch,e_test_batch)\n",
    "valid_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d35bb63",
   "metadata": {},
   "source": [
    "## COX比例风险模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "369a8da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-index: 0.5842646169664008\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.util import Surv\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "from sksurv.nonparametric import kaplan_meier_estimator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 将数据从张量转换为numpy数组\n",
    "X_batch_np = X_batch.numpy()\n",
    "y_batch_np = y_batch.numpy().flatten()\n",
    "e_batch_np = e_batch.numpy().flatten().astype(bool)  # 将事件指示转换为布尔类型\n",
    "\n",
    "# 创建生存数据结构\n",
    "surv_data = Surv.from_arrays(event=e_batch_np, time=y_batch_np)\n",
    "\n",
    "# 将特征矩阵转换为DataFrame\n",
    "X_batch_df = pd.DataFrame(X_batch_np)\n",
    "\n",
    "# 训练Cox比例风险模型\n",
    "cox = CoxPHSurvivalAnalysis()\n",
    "cox.fit(X_batch_df, surv_data)\n",
    "\n",
    "# 将测试数据从张量转换为numpy数组\n",
    "X_test_batch_np = X_test_batch.numpy()\n",
    "y_test_batch_np = y_test_batch.numpy().flatten()\n",
    "e_test_batch_np = e_test_batch.numpy().flatten().astype(bool)  # 将事件指示转换为布尔类型\n",
    "\n",
    "# 将特征矩阵转换为DataFrame\n",
    "X_test_batch_df = pd.DataFrame(X_test_batch_np)\n",
    "\n",
    "# 使用训练好的Cox模型进行预测\n",
    "predicted_survival = cox.predict_survival_function(X_test_batch_df)\n",
    "\n",
    "# 选择生存概率阈值 p\n",
    "p = 0.5\n",
    "\n",
    "# 计算每个样本在指定生存概率时的生存时间\n",
    "specified_surv_times = np.array([np.interp(p, sf.y[::-1], sf.x[::-1]) for sf in predicted_survival])\n",
    "\n",
    "# 计算C-index\n",
    "c_index = concordance_index_censored(e_test_batch_np, y_test_batch_np, -specified_surv_times)\n",
    "print(\"C-index:\", c_index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3202f97d",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d3f474b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-index: 0.5770601912028017\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sksurv.svm import FastSurvivalSVM\n",
    "from sksurv.util import Surv\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "\n",
    "# 假设你已经有了X_batch, y_batch和e_batch\n",
    "# 将数据从张量转换为numpy数组\n",
    "X_batch_np = X_batch.numpy()\n",
    "y_batch_np = y_batch.numpy().flatten()\n",
    "e_batch_np = e_batch.numpy().flatten().astype(bool)  # 将事件指示转换为布尔类型\n",
    "\n",
    "# 创建生存数据结构\n",
    "surv_data = Surv.from_arrays(event=e_batch_np, time=y_batch_np)\n",
    "\n",
    "# 训练FastKernelSurvivalSVM模型，选择多项式核\n",
    "svm = FastSurvivalSVM()\n",
    "svm.fit(X_batch_np, surv_data)\n",
    "\n",
    "# 将测试数据从张量转换为numpy数组\n",
    "X_test_batch_np = X_test_batch.numpy()\n",
    "y_test_batch_np = y_test_batch.numpy().flatten()\n",
    "e_test_batch_np = e_test_batch.numpy().flatten().astype(bool)  # 将事件指示转换为布尔类型\n",
    "# 使用训练好的模型进行预测\n",
    "predicted_risk = svm.predict(X_test_batch_np)\n",
    "\n",
    "# 计算C-index\n",
    "c_index = concordance_index_censored(e_test_batch_np, y_test_batch_np, predicted_risk)\n",
    "print(\"C-index:\", c_index[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25196ab7",
   "metadata": {},
   "source": [
    "# DeepNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "482cb7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class DeepNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(14, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3288018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regularization(object):\n",
    "    #### 初始化\n",
    "    def __init__(self, order):\n",
    "        ''' The initialization of Regularization class正则化类的初始化\n",
    "\n",
    "        :param order: (int) norm order number范数\n",
    "        :param weight_decay: (float) weight decay rate权重衰减率（权重衰降的强度）\n",
    "        :param p:默认求2范数，除非设定p=1（并未使用到）\n",
    "        '''\n",
    "        super(Regularization, self).__init__()  # 集成父类的属性和方法\n",
    "        # 将传递的属性保存在self中\n",
    "        self.order = order  # 范数\n",
    "        self.weight_decay = 0.001  # 权重衰减率\n",
    "\n",
    "    ### 用于计算模型的正则化损失\n",
    "    def __call__(self, model):  # model表示需要正则化的模型\n",
    "        ''' Performs calculates regularization(self.order) loss for model.\n",
    "\n",
    "        :param model: (torch.nn.Module object)\n",
    "        :return reg_loss: (torch.Tensor) the regularization(self.order) loss返回正则化的损失\n",
    "        '''\n",
    "        reg_loss = 0\n",
    "        for name, w in model.named_parameters():  # 遍历模型当中的参数\n",
    "            if 'weight' in name:  # 如果存在weight相关字眼，就证明时需要正则化的参数，那么就计算该参数的范数\n",
    "                reg_loss = reg_loss + torch.norm(w, p=self.order)  # torch.norm计算参数范数\n",
    "        reg_loss = self.weight_decay * reg_loss  # 最后将计算的范数乘以权重衰减率 self.weight_decay，得到正则化损失 reg_loss\n",
    "        return reg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33f98f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeLogLikelihood(nn.Module):\n",
    "    def __init__(self,weight_decay):\n",
    "        super(NegativeLogLikelihood, self).__init__()\n",
    "        self.L2_reg = 0.001\n",
    "        self.reg = Regularization(order=2, weight_decay=self.L2_reg)\n",
    "\n",
    "    def forward(self, risk_pred, y, e, model):\n",
    "        # 确定设备，这里假设所有输入张量都应该在同一个设备上\n",
    "        device = risk_pred.device\n",
    "        # 确保所有输入张量都在正确的设备上\n",
    "        y = y.to(device)\n",
    "        e = e.to(device)\n",
    "        model = model.to(device)  # 假设model也需要被移动到正确的设备\n",
    "\n",
    "        # 创建蒙版mask，确保它也在正确的设备上\n",
    "        mask = torch.ones(y.shape[0], y.shape[0], device=device)\n",
    "        mask[(y.T - y) > 0] = 0\n",
    "\n",
    "        # 确保风险预测指数化操作也在正确的设备上执行\n",
    "        log_loss = torch.exp(risk_pred.to(device)) * mask\n",
    "\n",
    "        # 接下来的操作...\n",
    "        log_loss = torch.sum(log_loss, dim=0) / torch.sum(mask, dim=0)\n",
    "        log_loss = torch.log(log_loss).reshape(-1, 1)\n",
    "\n",
    "        neg_log_loss = -torch.sum((risk_pred - log_loss) * e) / torch.sum(e)\n",
    "        l2_loss = self.reg(model)  # 确保正则化操作也在正确的设备上执行\n",
    "\n",
    "        return neg_log_loss + l2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cac8e7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepNN(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=14, out_features=32, bias=True)\n",
       "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.25, inplace=False)\n",
       "    (4): Linear(in_features=32, out_features=64, bias=True)\n",
       "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): Dropout(p=0.25, inplace=False)\n",
       "    (8): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (9): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): Dropout(p=0.25, inplace=False)\n",
       "    (12): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DeepNN()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ccbbcca0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Regularization.__init__() got an unexpected keyword argument 'weight_decay'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.047\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m criterion \u001b[38;5;241m=\u001b[39m \u001b[43mNegativeLogLikelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[1;32mIn[36], line 5\u001b[0m, in \u001b[0;36mNegativeLogLikelihood.__init__\u001b[1;34m(self, weight_decay)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m(NegativeLogLikelihood, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL2_reg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreg \u001b[38;5;241m=\u001b[39m \u001b[43mRegularization\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mL2_reg\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: Regularization.__init__() got an unexpected keyword argument 'weight_decay'"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.047)\n",
    "criterion = NegativeLogLikelihood(weight_decay=0.001).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c77cd05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch, lr):\n",
    "    ''' Adjusts learning rate according to (epoch, lr and lr_decay_rate)\n",
    "\n",
    "    :param optimizer: (torch.optim object) 优化器\n",
    "    :param epoch: (int) 迭代次数\n",
    "    :param lr: (float) the initial learning rate 学习率\n",
    "    :param lr_decay_rate: (float) learning rate decay rate 学习率衰减率\n",
    "    :return lr_: (float) updated learning rate 返回一个更新好的学习率\n",
    "    '''\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr / (1 + epoch * 2.573e-3)\n",
    "    return optimizer.param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7155e116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader, test_loader, device):\n",
    "    ''' Performs training according to .ini file\n",
    "\n",
    "    :param ini_file: (String) the path of .ini file 每个数据集的参数配置文件\n",
    "    :return best_c_index: the best c-index 最优的c指数\n",
    "    '''\n",
    "    # training\n",
    "    best_c_index = 0  # 定义一个变量 best_c_index，用于记录训练过程中最优的 c-index\n",
    "    flag = 0  # 定义一个变量 flag，用于记录训练过程中最优的 c-index 持续多少个 epoch 未更新，以便进行早停策略\n",
    "    for epoch in range(1, 501):\n",
    "        # adjusts learning rate\n",
    "        lr = adjust_learning_rate(optimizer, epoch, 0.047 )\n",
    "        # train step\n",
    "        model.train()  # 模型调整到训练模式\n",
    "        for X, y, e in train_loader:  # 整个过程重复，直到处理完train_loader中所有数据\n",
    "            # 将数据移动到指定设备\n",
    "            X, y, e = X.to(device), y.to(device), e.to(device)\n",
    "            # makes predictions 做预测\n",
    "            risk_pred = model(X)  # 风险预测值\n",
    "            train_loss = criterion(risk_pred, y, e, model)  # 训练损失函数\n",
    "            train_c = c_index(-risk_pred, y, e)  # 训练出来的c指数\n",
    "            # updates parameters 更新参数\n",
    "            optimizer.zero_grad()  # 清空当前的梯度\n",
    "            train_loss.backward()  # 计算损失函数的梯度\n",
    "            optimizer.step()  # 更新模型\n",
    "        \n",
    "        # valid step\n",
    "        model.eval()  # 模型调整到测试模式\n",
    "        for X, y, e in test_loader:\n",
    "            # 将数据移动到指定设备\n",
    "            X, y, e = X.to(device), y.to(device), e.to(device)\n",
    "            # makes predictions 做预测\n",
    "            with torch.no_grad():  # 因为测试模式下，无需计算梯度，因此关闭梯度计算，以提高计算速度\n",
    "                risk_pred = model(X)  # 风险预测\n",
    "                valid_loss = criterion(risk_pred, y, e, model)  # 计算损失函数\n",
    "                valid_c = c_index(-risk_pred, y, e)  # 测试的c指数\n",
    "                if best_c_index < valid_c:\n",
    "                    best_c_index = valid_c\n",
    "                    flag = 0  # 如果当前c指数>最大c指数，更新c指数，flag=0\n",
    "                    # saves the best model 保存预测效果最好的那一次迭代\n",
    "                    torch.save({  # 并且将当前的模型保存下来\n",
    "                        'model': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'epoch': epoch}, 'test.pth')\n",
    "                else:\n",
    "                    flag += 1\n",
    "                    if flag >= 50:\n",
    "                        print(f\"Early stopping at epoch {epoch}, best c-index: {best_c_index}\")\n",
    "                        return best_c_index\n",
    "        # notes that, train loader and valid loader both have one batch!!!注意，train loader和valid loader都有一个批次!!\n",
    "        print('\\rEpoch: {}\\tLoss: {:.8f}({:.8f})\\tc-index: {:.8f}({:.8f})\\tlr: {:g}'.format(\n",
    "            epoch, train_loss.item(), valid_loss.item(), train_c, valid_c, lr), end='', flush=False)\n",
    "        print(f\"\\nFlag: {flag}, Best c-index: {best_c_index}\")\n",
    "    return best_c_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "586b8608",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'criterion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m best_c_index \u001b[38;5;241m=\u001b[39m train(model,\u001b[43mcriterion\u001b[49m,optimizer,train_loader,test_loader,device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'criterion' is not defined"
     ]
    }
   ],
   "source": [
    "best_c_index = train(model,criterion,optimizer,train_loader,test_loader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114a05c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
